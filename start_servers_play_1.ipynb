{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "start-servers-play - 1.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sssharaf/torch_hub/blob/master/start_servers_play_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5lt8ZaM-nCy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "!ls -ltr /gdrive/'My Drive'/ML/data/start-servers-play\n",
        "!pip install pytorch_transformers\n",
        "!ln -s  /gdrive/'My Drive'/ML/data/start-servers-play data\n",
        "!ls -ltr data/*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjxaBXQ6_z49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "import pytorch_transformers as pt\n",
        "from pytorch_transformers import BertTokenizer, BertConfig,BertForMaskedLM,BertModel,DistilBertTokenizer, DistilBertModel,DistilBertForSequenceClassification \n",
        "import os\n",
        "import typing\n",
        "from typing import Dict,List,Sequence,Set\n",
        "from types import SimpleNamespace as SN\n",
        "import numpy as np\n",
        "import pickle\n",
        "import math\n",
        "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight,compute_sample_weight\n",
        "T_BertTokenizer = typing.NewType(\"BertTokenizer\",BertTokenizer)\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziNHWdP8_8UQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AExKHXwAJlG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn_df = pd.read_csv('data/train.csv',dtype={'action':'category','component':'category'},)\n",
        "val_df = pd.read_csv('data/val-new.csv',dtype={'action':'category','component':'category'})\n",
        "#trn_df.loc[trn_df.action=='noaction']\n",
        "trn_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYhoktKLPhdk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-PHDUlxAkSd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "action_le = LabelEncoder()\n",
        "action_le.fit(trn_df.action)\n",
        "component_le = LabelEncoder()\n",
        "component_le.fit(trn_df.component)\n",
        "print(action_le.classes_)\n",
        "print(component_le.classes_)\n",
        "\n",
        "with open(f'data/action_le.dat','wb') as f:\n",
        "  pickle.dump(action_le,f)\n",
        "\n",
        "with open(f'data/component_le.dat','wb') as f:\n",
        "  pickle.dump(component_le,f)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPyjvoDcAa6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def encode_X(comment:str,max_len):\n",
        "  X = f\"[CLS] {comment} [SEP]\"\n",
        "  encoded = torch.tensor(tokenizer.encode(X),dtype=torch.long)\n",
        "  X = torch.zeros(max_len,dtype=torch.long)\n",
        "  X[:len(encoded)] = encoded\n",
        "  X[len(encoded)+1:] = torch.tensor(tokenizer.pad_token_id,dtype=torch.long)  \n",
        "  X_attn_mask = X!=tokenizer.pad_token_id\n",
        "  X_attn_mask = X_attn_mask.int()\n",
        "  return X,X_attn_mask\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "\n",
        "    def __init__(self,df:DataFrame,max_len = 16):\n",
        "        self.df = df\n",
        "        self.max_len=max_len\n",
        "        self.action = self.df.action.cat.codes\n",
        "        self.component = self.df.component.cat.codes\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        X = self.df.iloc[index]['comment_text']\n",
        "        X,X_attn_mask = encode_X(X,self.max_len)\n",
        "        Y1 = self.df.iloc[index]['action']\n",
        "        Y1 = action_le.transform([Y1])\n",
        "        #Y1 = a_ohe.transform([[Y1]])\n",
        "        Y1 = torch.tensor(Y1,dtype=torch.long)\n",
        "        Y2 = self.df.iloc[index]['component']\n",
        "        Y2 = component_le.transform([Y2])\n",
        "        #Y2 = c_ohe.transform([[Y2]])\n",
        "        Y2 = torch.tensor(Y2, dtype=torch.long)\n",
        "        return (X,X_attn_mask),(Y1.squeeze(),Y2.squeeze())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def components(self):\n",
        "        return self.component"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPDP-89dCVOQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "trn_ds = MyDataset(trn_df,max_len=25)\n",
        "val_ds = MyDataset(val_df,max_len=25)\n",
        "\n",
        "trn_dl = DataLoader(dataset=trn_ds,batch_size=16,pin_memory=True,shuffle=True)\n",
        "val_dl = DataLoader(dataset=val_ds,batch_size=16,pin_memory=True,shuffle=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXTXXGTiCzmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(trn_ds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQUqP1PEGuFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model\n",
        "class MyModel3(nn.Module):\n",
        "  def __init__(self, freeze_bert = True):\n",
        "    super().__init__()\n",
        "    self.model_version = 3\n",
        "\n",
        "    self.bert_lyr = BertModel.from_pretrained('bert-base-uncased',output_hidden_states=True)\n",
        "    \n",
        "    self.a_attn = nn.Linear(768,1,bias=False)\n",
        "    \n",
        "    self.c_attn = nn.Linear(768,1,bias=False)\n",
        "\n",
        "    self.attn_dropout = nn.Dropout(0.1,inplace=False)\n",
        "    \n",
        "    self.a_switch = nn.Sequential(\n",
        "        nn.Dropout(0.1),\n",
        "        nn.Linear(768*2,64),\n",
        "        nn.ELU(),\n",
        "        nn.LayerNorm(64),\n",
        "        nn.Dropout(0.1),\n",
        "        nn.Linear(64,1),\n",
        "        nn.Sigmoid(),\n",
        "        )\n",
        "    \n",
        "    self.c_switch = nn.Sequential(\n",
        "        nn.Dropout(0.1),\n",
        "        nn.Linear(768*2,64),\n",
        "        nn.ELU(),\n",
        "        nn.LayerNorm(64),\n",
        "        nn.Dropout(0.1),\n",
        "        nn.Linear(64,1),\n",
        "        nn.Sigmoid(),\n",
        "    )\n",
        "\n",
        "    self.action_cls_lyr = nn.Sequential(\n",
        "        nn.Dropout(0.1),\n",
        "        nn.Linear(768,64,bias=False),\n",
        "        nn.ELU(),\n",
        "        nn.LayerNorm(64),\n",
        "        nn.Dropout(0.1),\n",
        "        nn.Linear(64,len(action_le.classes_),bias=False),   \n",
        "    )\n",
        "\n",
        "    self.component_cls_lyr = nn.Sequential(\n",
        "                nn.Dropout(0.1),\n",
        "                nn.Linear(768,64,bias=False),\n",
        "                nn.ELU(),\n",
        "                nn.LayerNorm(64),\n",
        "                nn.Dropout(0.1),\n",
        "                nn.Linear(64,len(component_le.classes_),bias=False),\n",
        "    )\n",
        "    \n",
        "    #Freeze bert layers\n",
        "    if freeze_bert:\n",
        "       for p in self.bert_lyr.parameters():#self.bert_lyr.parameters():\n",
        "              p.requires_grad = False\n",
        "    #nn.init.xavier_uniform_(self.action_cls_lyr)\n",
        "    #nn.init.xavier_uniform_(self.component_cls_lyr)\n",
        "\n",
        "  def forward(self, seq, attn_masks,output_attn=False,output_hs=False,output_switch=False):\n",
        "    attn_mask_cls = (1 - attn_masks)*-10000\n",
        "    attn_mask_cls.unsqueeze_(dim=-1)\n",
        "\n",
        "    seq_emb,ctx,hs = self.bert_lyr(seq,attention_mask =attn_masks)\n",
        "   \n",
        "    a = self.a_attn(seq_emb)\n",
        "    a = a + attn_mask_cls\n",
        "    a =  a_output = a.softmax(dim=1)\n",
        "    a = self.attn_dropout(a)\n",
        "    a = torch.mul(seq_emb,a)\n",
        "    a = a.mean(dim=1)\n",
        "\n",
        "    c = self.c_attn(seq_emb)\n",
        "    c = c + attn_mask_cls\n",
        "    c =  c_output = c.softmax(dim=1)\n",
        "    c = self.attn_dropout(c)\n",
        "    c = torch.mul(seq_emb,c)\n",
        "    c = c.mean(dim=1)\n",
        "\n",
        "    a_switch = self.a_switch(torch.cat([ctx.detach(),a],dim=-1))    \n",
        "    a = (1 - a_switch)*a + a_switch*ctx\n",
        "\n",
        "    c_switch = self.c_switch(torch.cat([ctx.detach(),c],dim=-1))\n",
        "    c = (1 - c_switch)*c + c_switch*ctx\n",
        "\n",
        "    outputs = [self.action_cls_lyr(a),self.component_cls_lyr(c)]\n",
        "    if (output_attn):\n",
        "      outputs += [a_output,c_output]\n",
        "    if output_hs:\n",
        "      outputs +=[hs]\n",
        "    if output_switch:\n",
        "      outputs +=[a_switch,c_switch]\n",
        "    return outputs\n",
        "\n",
        "############################### Model 2  ############################################\n",
        "# Model\n",
        "class MyModel2(nn.Module):\n",
        "  def __init__(self, freeze_bert = True):\n",
        "    super().__init__()\n",
        "    self.model_version = 2-1\n",
        "    #self.static_bert_lyr = BertModel.from_pretrained('bert-base-uncased',output_hidden_states=False)\n",
        "    self.bert_lyr = BertModel.from_pretrained('bert-base-uncased',output_hidden_states=True)\n",
        "    \n",
        "    self.a_attn = nn.Linear(768,1)\n",
        "    \n",
        "    self.c_attn = nn.Linear(768,1)\n",
        "    \n",
        "    self.action_cls_lyr = nn.Sequential(\n",
        "        nn.Dropout(0.1),\n",
        "        nn.Linear(768,64,bias=True),\n",
        "        nn.LayerNorm(64),\n",
        "        nn.ELU(),\n",
        "        nn.Linear(64,len(action_le.classes_),bias=True),\n",
        "        nn.LayerNorm(len(action_le.classes_)),\n",
        "        #nn.Linear(768,len(action_le.classes_),bias=False),      \n",
        "    )\n",
        "\n",
        "    self.component_cls_lyr = nn.Sequential(\n",
        "                nn.Dropout(0.1),\n",
        "                nn.Linear(768,64,bias=True),\n",
        "                nn.LayerNorm(64),\n",
        "                nn.ELU(),\n",
        "                nn.Linear(64,len(component_le.classes_),bias=True),\n",
        "                nn.LayerNorm(len(component_le.classes_)),\n",
        "                #nn.Linear(768,len(component_le.classes_),bias=False),\n",
        "    )\n",
        "    \n",
        "    # for p in self.static_bert_lyr.parameters():\n",
        "    #   p.requires_grad = False\n",
        "\n",
        "    #Freeze bert layers\n",
        "    if freeze_bert:\n",
        "        for lyr in self.bert_lyr.encoder.layer[:-2]:\n",
        "          for p in lyr.parameters():#self.bert_lyr.parameters():\n",
        "              p.requires_grad = False\n",
        "    #nn.init.xavier_uniform_(self.action_cls_lyr)\n",
        "    #nn.init.xavier_uniform_(self.component_cls_lyr)\n",
        "\n",
        "  def forward(self, seq, attn_masks,output_attn=False,output_hs=False):\n",
        "    attn_mask_cls = (1 - attn_masks)*-10000\n",
        "    attn_mask_cls.unsqueeze_(dim=-1)\n",
        "\n",
        "    #static_emb,static_ctx = self.static_bert_lyr(seq,attention_mask =attn_masks)\n",
        "    seq_emb,ctx,hs = self.bert_lyr(seq,attention_mask =attn_masks)\n",
        "    #seq_emb +=static_emb\n",
        "    a = self.a_attn(seq_emb)\n",
        "    a = a + attn_mask_cls\n",
        "    a = a_output = a.softmax(dim=1)\n",
        "    a = torch.mul(seq_emb,a)\n",
        "    a = a.mean(dim=1)\n",
        "\n",
        "    c = self.c_attn(seq_emb)\n",
        "    c = c + attn_mask_cls\n",
        "    c = c_output =  c.softmax(dim=1)\n",
        "    c = torch.mul(seq_emb,c)\n",
        "    c = c.mean(dim=1)\n",
        "\n",
        "    outputs = [self.action_cls_lyr(a),self.component_cls_lyr(c)]\n",
        "    if (output_attn):\n",
        "      outputs += [a_output,c_output]\n",
        "    if output_hs:\n",
        "      outputs +=[hs]\n",
        "    return outputs\n",
        "#####################################################################################\n",
        "model = MyModel3(freeze_bert=False)\n",
        "model = model.to(DEVICE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15EENDfO9JAG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pytorch_transformers import BertForSequenceClassification\n",
        "############################### Model 1  ############################################\n",
        "# Model\n",
        "class MyModel4(nn.Module):\n",
        "  def __init__(self, freeze_bert = True):\n",
        "    super().__init__()\n",
        "    self.model_version = 4\n",
        "    \n",
        "    self.bert_lyr = BertModel.from_pretrained('bert-base-uncased',output_hidden_states=True,output_attentions=True)\n",
        "\n",
        "    #self.comp_bert_lyr = BertModel.from_pretrained('bert-base-uncased',output_hidden_states=True,output_attentions=True)\n",
        "    \n",
        "    self.config = self.bert_lyr.config;\n",
        "\n",
        "    self.action_cls_lyr = nn.Sequential(\n",
        "            nn.LayerNorm(768),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(768,64),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(64),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(64,4),\n",
        "    )\n",
        "    self.comp_cls_lyr = nn.Sequential(\n",
        "            nn.LayerNorm(768),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(768,64),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(64),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(64,5),\n",
        "        )    \n",
        "\n",
        "    #Freeze bert layers\n",
        "    if freeze_bert:\n",
        "        self.freeze_bert()\n",
        "    else:\n",
        "        pass\n",
        "        self.freeze_bert()\n",
        "        self.unfreeze_bert()\n",
        "        \n",
        "  def freeze_bert(self):\n",
        "      for p in self.bert_lyr.parameters():\n",
        "        p.requires_grad = False\n",
        "      # for p in self.comp_bert_lyr.parameters():\n",
        "      #   p.requires_grad = False\n",
        "\n",
        "  def unfreeze_bert(self,from_lyr=6):\n",
        "\n",
        "    for lyr in self.bert_lyr.encoder.layer[-11:]:\n",
        "      for p in lyr.parameters():\n",
        "        p.requires_grad = True\n",
        "    # for lyr in self.comp_bert_lyr.encoder.layer[-6:]:\n",
        "    #   for p in lyr.parameters():\n",
        "    #     p.requires_grad = True\n",
        "\n",
        "  def forward(self, seq, attn_masks,output_attn=False,output_hs=False):\n",
        "    \n",
        "    seq_emb,pooled,hs,attn = self.bert_lyr(seq,attention_mask =attn_masks)\n",
        "    \n",
        "    #c_seq_emb,c_pooled,c_hs,c_attn = self.comp_bert_lyr(seq,attention_mask =attn_masks)\n",
        "    \n",
        "    outputs=[]\n",
        "    outputs += [\n",
        "                self.action_cls_lyr(pooled),\n",
        "                self.comp_cls_lyr(pooled),\n",
        "                ]\n",
        "    return outputs\n",
        "\n",
        "model = MyModel4(freeze_bert=False)\n",
        "model = model.to(DEVICE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvE6VXRMOilX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pytorch_transformers import BertForSequenceClassification\n",
        "############################### Model 1  ############################################\n",
        "# Model\n",
        "class MyModel5(nn.Module):\n",
        "  def __init__(self, freeze_bert = True):\n",
        "    super().__init__()\n",
        "    self.model_version = 5\n",
        "    \n",
        "    self.bert_lyr = BertModel.from_pretrained('bert-base-uncased',output_hidden_states=True,output_attentions=True)\n",
        "\n",
        "    self.config = self.bert_lyr.config;\n",
        "\n",
        "    self.cls_lyr = nn.Sequential(\n",
        "            nn.LayerNorm(768),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(768,64),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(64),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(64,9),\n",
        "    )\n",
        "\n",
        "    #Freeze bert layers\n",
        "    if freeze_bert:\n",
        "        self.freeze_bert()\n",
        "    else:\n",
        "        pass\n",
        "        self.freeze_bert()\n",
        "        self.unfreeze_bert()\n",
        "        \n",
        "  def freeze_bert(self):\n",
        "      for p in self.bert_lyr.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "  def unfreeze_bert(self,from_lyr=6):\n",
        "\n",
        "    for lyr in self.bert_lyr.encoder.layer[-11:]:\n",
        "      for p in lyr.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "\n",
        "  def forward(self, seq, attn_masks,output_attn=False,output_hs=False):\n",
        "    \n",
        "    seq_emb,pooled,hs,attn = self.bert_lyr(seq,attention_mask =attn_masks)\n",
        "    o=self.cls_lyr(pooled)     \n",
        "    outputs=[]\n",
        "    outputs += [\n",
        "                o[:,:4],\n",
        "                o[:,4:],\n",
        "                ]\n",
        "    return outputs\n",
        "\n",
        "model = MyModel5(freeze_bert=False)\n",
        "model = model.to(DEVICE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8Yf8oSUTGzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pytorch_transformers import BertForSequenceClassification\n",
        "############################### Model 1  ############################################\n",
        "# Model\n",
        "class MyModel7(nn.Module):\n",
        "  def __init__(self, freeze_bert = True):\n",
        "    super().__init__()\n",
        "    self.model_version = '7_1'\n",
        "    \n",
        "    self.bert_lyr = BertModel.from_pretrained('bert-base-uncased',output_hidden_states=True,output_attentions=True)\n",
        "\n",
        "    #self.comp_bert_lyr = BertModel.from_pretrained('bert-base-uncased',output_hidden_states=True,output_attentions=True)\n",
        "    \n",
        "    self.config = self.bert_lyr.config;\n",
        "\n",
        "    self.a_attn = nn.Linear(768,1)\n",
        "    \n",
        "    self.c_attn = nn.Linear(768,1)\n",
        "\n",
        "    self.action_cls_lyr = nn.Sequential(\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(768,4),\n",
        "    )\n",
        "    self.comp_cls_lyr = nn.Sequential(\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(768,5),\n",
        "        )    \n",
        "\n",
        "    #Freeze bert layers\n",
        "    if freeze_bert:\n",
        "        self.freeze_bert()\n",
        "    else:\n",
        "        self.freeze_bert()\n",
        "        self.unfreeze_bert()\n",
        "        \n",
        "  def freeze_bert(self):\n",
        "      for p in self.bert_lyr.parameters():\n",
        "        p.requires_grad = False\n",
        "      # for p in self.comp_bert_lyr.parameters():\n",
        "      #   p.requires_grad = False\n",
        "\n",
        "  def unfreeze_bert(self,from_lyr=6):\n",
        "\n",
        "    for lyr in self.bert_lyr.encoder.layer[-6:]:\n",
        "      for p in lyr.parameters():\n",
        "        p.requires_grad = True\n",
        "    # for lyr in self.comp_bert_lyr.encoder.layer[-6:]:\n",
        "    #   for p in lyr.parameters():\n",
        "    #     p.requires_grad = True\n",
        "\n",
        "\n",
        "  def forward(self, seq, attn_masks,output_attn=False,output_hs=False):\n",
        "    attn_mask_cls = (1 - attn_masks)*-10000\n",
        "    attn_mask_cls.unsqueeze_(dim=-1)\n",
        "    seq_emb,pooled,hs,attn = self.bert_lyr(seq,attention_mask =attn_masks)\n",
        "    \n",
        "    #c_seq_emb,c_pooled,c_hs,c_attn = self.comp_bert_lyr(seq,attention_mask =attn_masks)\n",
        "    a,a_output = self.attention(seq_emb,self.a_attn,attn_mask_cls)\n",
        "\n",
        "    c,c_output = self.attention(seq_emb,self.c_attn,attn_mask_cls)\n",
        "\n",
        "    a_pooled  = a\n",
        "    c_pooled  = c\n",
        "    outputs=[]\n",
        "    outputs += [\n",
        "                self.action_cls_lyr(a_pooled),\n",
        "                self.comp_cls_lyr(c_pooled),\n",
        "                ]\n",
        "    if (output_attn):\n",
        "      outputs += [a_output,c_output]\n",
        "    return outputs\n",
        "\n",
        "  def attention(self,seq_emb,attn_lyr,attn_mask_cls):\n",
        "    a = attn_lyr(seq_emb)\n",
        "    a = a + attn_mask_cls\n",
        "    a = a_output = a.softmax(dim=1)\n",
        "    #a_output = a.clone()\n",
        "    a = torch.mul(seq_emb,a)\n",
        "    a = a.mean(dim=1)\n",
        "    return a,a_output\n",
        "\n",
        "try:\n",
        "  del model\n",
        "  torch.cuda.reset_max_memory_allocated()\n",
        "  torch.cuda.reset_max_memory_cached()\n",
        "except NameError:\n",
        "  pass\n",
        "\n",
        "model = MyModel7(freeze_bert=False)\n",
        "model = model.to(DEVICE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l017tvps2fJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pytorch_transformers import BertForSequenceClassification\n",
        "############################### Model 1  ############################################\n",
        "# Model\n",
        "class MyModel8_LSTM(nn.Module):\n",
        "  def __init__(self, freeze_bert = True):\n",
        "    super().__init__()\n",
        "    self.model_version = '8'\n",
        "    \n",
        "    self.bert_lyr = BertModel.from_pretrained('bert-base-uncased',output_hidden_states=True,output_attentions=True)\n",
        "    \n",
        "    self.config = self.bert_lyr.config;\n",
        "\n",
        "    n_features = 64\n",
        "    self.a_attn = nn.Linear(768,1)\n",
        "    \n",
        "    self.c_attn = nn.Linear(768,1)\n",
        "\n",
        "    self.action_cls_lyr = nn.Sequential(\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(n_features,4),\n",
        "    )\n",
        "    self.comp_cls_lyr = nn.Sequential(\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(n_features,5),\n",
        "        )    \n",
        "\n",
        "    self.a_lstm = nn.LSTM(batch_first=True,input_size=768,hidden_size=n_features,num_layers=1)\n",
        "    self.c_lstm = nn.LSTM(batch_first=True,input_size=768,hidden_size=n_features,num_layers=1)\n",
        "    \n",
        "    #Freeze bert layers\n",
        "    if freeze_bert:\n",
        "        self.freeze_bert()\n",
        "    else:\n",
        "        self.freeze_bert()\n",
        "        self.unfreeze_bert()\n",
        "        \n",
        "  def freeze_bert(self):\n",
        "      for p in self.bert_lyr.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "  def unfreeze_bert(self,from_lyr=6):\n",
        "\n",
        "    for lyr in self.bert_lyr.encoder.layer[-6:]:\n",
        "      for p in lyr.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "\n",
        "  def attention(self,seq_emb,attn_lyr,attn_mask_cls):\n",
        "    a = attn_lyr(seq_emb)\n",
        "    a = a + attn_mask_cls\n",
        "    a = a_output = a.softmax(dim=1)\n",
        "    a = torch.mul(seq_emb,a)\n",
        "    #a = a.mean(dim=1)\n",
        "    return a,a_output\n",
        "\n",
        "\n",
        "  def forward(self, seq, attn_masks,output_attn=False,output_hs=False):\n",
        "    attn_mask_cls = (1 - attn_masks)*-10000\n",
        "    attn_mask_cls.unsqueeze_(dim=-1)\n",
        "    seq_emb,pooled,hs,attn = self.bert_lyr(seq,attention_mask =attn_masks)\n",
        "    #seq_emb, (_,_) = self.lstm(seq_emb)\n",
        "    a,a_output = self.attention(seq_emb,self.a_attn,attn_mask_cls)\n",
        "    _, (a,_) = self.a_lstm(a)\n",
        "    a=a.sum(dim=0)\n",
        "    c,c_output = self.attention(seq_emb,self.c_attn,attn_mask_cls)\n",
        "    _, (c,_) = self.c_lstm(c)\n",
        "    c=c.sum(dim=0)\n",
        "\n",
        "    a_pooled  = a\n",
        "    c_pooled  = c\n",
        "    outputs=[]\n",
        "    outputs += [\n",
        "                self.action_cls_lyr(a_pooled),\n",
        "                self.comp_cls_lyr(c_pooled),\n",
        "                ]\n",
        "    if (output_attn):\n",
        "      outputs += [a_output,c_output]\n",
        "    return outputs\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "  del model\n",
        "  torch.cuda.reset_max_memory_allocated()\n",
        "  torch.cuda.reset_max_memory_cached()\n",
        "except NameError:\n",
        "  pass\n",
        "\n",
        "model = MyModel8(freeze_bert=False)\n",
        "model = model.to(DEVICE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oft4yA2YeQoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i = torch.rand(16,25,768)\n",
        "attn = nn.Linear(384,1)\n",
        "\n",
        "i2 = i.view(16,25,2,-1).contiguous()\n",
        "print(f'i2.size()={i2.size()}')\n",
        "alpha = attn(i2)\n",
        "alpha= alpha.softmax(dim=-3)\n",
        "print(alpha.size())\n",
        "att_s = i2 * alpha\n",
        "print(f'alpha_size={alpha.size()}')\n",
        "alpha[0,:,1,0].sum()\n",
        "print(f'att_s size = {att_s.size()}')\n",
        "#print(att_s.view(16,25,-1).size())\n",
        "att_s_ctx = att_s.sum(dim=1)\n",
        "print(att_s_ctx.view(16,-1).size())\n",
        "#alpha.sum(dim=-2).squeeze().size()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLsZhocod-wL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pytorch_transformers import BertForSequenceClassification\n",
        "############################### Model 1  ############################################\n",
        "# With different attention\n",
        "class MyModel9(nn.Module):\n",
        "  def __init__(self, freeze_bert = True, attns:int=1, attn_dropout=0.1):\n",
        "    super().__init__()\n",
        "    attns = int(attns)\n",
        "\n",
        "    self.model_version = '9'\n",
        "    \n",
        "    self.bert_lyr = BertModel.from_pretrained('bert-base-uncased',output_hidden_states=True,output_attentions=True)\n",
        "    \n",
        "    self.config = self.bert_lyr.config;\n",
        "\n",
        "    self.attns = attns\n",
        "\n",
        "    self.a_attn_vec = nn.Parameter(torch.Tensor(768))\n",
        "    \n",
        "    self.c_attn_vec = nn.Parameter(torch.Tensor(768))\n",
        "\n",
        "    nn.init.normal_(self.a_attn_vec)\n",
        "    nn.init.normal_(self.c_attn_vec)\n",
        "\n",
        "    self.attn_dropout = nn.Dropout(attn_dropout)\n",
        "    \n",
        "    assert 768 % attns == 0\n",
        "    \n",
        "    att_hs = int(768 / attns)\n",
        "    \n",
        "    # self.a_attn = nn.Linear(att_hs,1)\n",
        "    \n",
        "    # self.c_attn = nn.Linear(att_hs,1)\n",
        "\n",
        "    self.action_cls_lyr = nn.Sequential(\n",
        "            nn.LayerNorm(768),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(768,64),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(64),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(64,4),\n",
        "    )\n",
        "    self.comp_cls_lyr = nn.Sequential(\n",
        "            nn.LayerNorm(768),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(768,64),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(64),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(64,5),\n",
        "        )    \n",
        "\n",
        "    #Freeze bert layers\n",
        "    if freeze_bert:\n",
        "        self.freeze_bert()\n",
        "    else:\n",
        "        self.freeze_bert()\n",
        "        self.unfreeze_bert()\n",
        "        \n",
        "  def freeze_bert(self):\n",
        "      for p in self.bert_lyr.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "  def unfreeze_bert(self,from_lyr=6):\n",
        "\n",
        "    for lyr in self.bert_lyr.encoder.layer[-6:]:\n",
        "      for p in lyr.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "  def attention_base(self,seq_emb,attn_lyr,attn_mask_cls):\n",
        "      attn_mask_cls = attn_mask_cls.unsqueeze(dim=-1)\n",
        "      a = attn_lyr(seq_emb)\n",
        "      a = a + attn_mask_cls\n",
        "      a = a_output = a.softmax(dim=1)\n",
        "      a = torch.mul(seq_emb,a)\n",
        "      a = a.mean(dim=1)\n",
        "      return a,a_output\n",
        "\n",
        "  def attention(self,seq_emb,attn_lyr,attn_mask_cls):\n",
        "    attns = self.attns\n",
        "    bs,ss,es = seq_emb.size()\n",
        "    #(s,w,d) -> (s,w,a,d/a)\n",
        "    seq_emb_t = seq_emb.view(bs,ss,attns,-1).contiguous()\n",
        "    if attns > 1:\n",
        "        attn_mask_cls =attn_mask_cls.repeat(1,attns).view(bs,attns,-1).permute(0,-1,-2).contiguous()\n",
        "    else:\n",
        "      attn_mask_cls = attn_mask_cls.unsqueeze(dim=-1)\n",
        "    # (s,w,a,d/a) -> (s,w,a,d/a,1)\n",
        "    attn_mask_cls = attn_mask_cls.unsqueeze(dim=-1)\n",
        "\n",
        "    # (s,w,a,d/a) -> (s,w,a,1)\n",
        "    alpha = attn_lyr(seq_emb_t)\n",
        "    alpha += attn_mask_cls\n",
        "    alpha = alpha.softmax(dim=-3)\n",
        "    alpha = self.attn_dropout(alpha)\n",
        "    #(s,w,a,d/a)\n",
        "    att_s = torch.mul(seq_emb_t, alpha)\n",
        "    att_s_ctx = att_s.sum(dim=1).view(bs,-1).contiguous()\n",
        "    alpha = alpha.squeeze(dim=-1).sum(dim=-1)\n",
        "    return att_s_ctx,alpha.squeeze(dim=-1)\n",
        "\n",
        "  def attention2(self,seq_emb,attn_vec,attn_mask_cls):\n",
        "    bs,ss,es = seq_emb.size()\n",
        "    seq_emb_t = seq_emb.view(bs,ss,self.attns,-1)\n",
        "    #(s,w,a,d/a) -> (s,a,w,d/a)\n",
        "    seq_emb_t = seq_emb_t.permute(0,2,1,3)\n",
        "\n",
        "    #(d) -> (a,d/a,1)\n",
        "    attn = attn_vec.view(self.attns,-1).unsqueeze(dim=-1)\n",
        "    #(s,a,w,1)\n",
        "    alpha = torch.matmul(seq_emb_t,attn)\n",
        "    alpha /= math.sqrt(es)    \n",
        "    attn_mask_cls = attn_mask_cls.unsqueeze(dim=1)\n",
        "    attn_mask_cls = torch.cat([attn_mask_cls]*self.attns,dim=1)\n",
        "    attn_mask_cls = attn_mask_cls.unsqueeze(dim=-1)\n",
        "    #print(f'alpha.size()={alpha.size()}')\n",
        "    #print(f'attn_mask_cls.size()={attn_mask_cls.size()}')\n",
        "    alpha  = alpha + attn_mask_cls\n",
        "    alpha = alpha.softmax(dim=2)\n",
        "    alpha = self.attn_dropout(alpha)\n",
        "    att_s = seq_emb_t*alpha\n",
        "    att_s = att_s.sum(dim=-2)\n",
        "    att_s = att_s.view(bs,-1).contiguous()\n",
        "    return att_s,alpha.mean(dim=1).squeeze()\n",
        "\n",
        "  def forward(self, seq, attn_masks,output_attn=False,output_hs=False):\n",
        "    attn_mask_cls = (1 - attn_masks)*-10000\n",
        "    #attn_mask_cls.unsqueeze_(dim=-1)\n",
        "    seq_emb,pooled,hs,attn = self.bert_lyr(seq,attention_mask =attn_masks)\n",
        "    \n",
        "    #a,a_output = self.attention(seq_emb,self.a_attn,attn_mask_cls)\n",
        "    a,a_output = self.attention2(seq_emb,self.a_attn_vec,attn_mask_cls)\n",
        "\n",
        "    #c,c_output = self.attention(seq_emb,self.c_attn,attn_mask_cls)\n",
        "    c,c_output = self.attention2(seq_emb,self.c_attn_vec,attn_mask_cls)\n",
        "\n",
        "    a_pooled  = a\n",
        "    c_pooled  = c\n",
        "    outputs=[]\n",
        "    outputs += [\n",
        "                self.action_cls_lyr(a_pooled),\n",
        "                self.comp_cls_lyr(c_pooled),\n",
        "                ]\n",
        "    if (output_attn):\n",
        "      outputs += [a_output,c_output]\n",
        "    return outputs\n",
        "\n",
        "\n",
        "# try:\n",
        "#   del model\n",
        "#   torch.cuda.reset_max_memory_allocated()\n",
        "#   torch.cuda.reset_max_memory_cached()\n",
        "# except NameError:\n",
        "#   pass\n",
        "\n",
        "# model = MyModel9(freeze_bert=False,attns=3)\n",
        "# model = model.to(DEVICE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxZ9Oo4NSNHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i = torch.rand(2,3,768,device=DEVICE)\n",
        "attn_mask = torch.zeros(2,3,device=DEVICE)\n",
        "attns = 2\n",
        "assert 768%attns == 0\n",
        "attn_vec = torch.rand(768)\n",
        "attn_vec = attn_vec.to(DEVICE)\n",
        "torch.nn.init.uniform_(attn_vec)\n",
        "#attn_lyr = nn.Linear(int(768/attns),1)\n",
        "attn_lyr = attn_lyr.to(DEVICE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnwTCsoH_KDi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "i = torch.rand(2,3,768,device=DEVICE)\n",
        "attn_mask = torch.zeros(2,3,device=DEVICE)\n",
        "\n",
        "attn_mask[0,0]=-10000\n",
        "print(attn_mask)\n",
        "\n",
        "# #attn_lyr= model.a_attn\n",
        "# seq_emb = i.clone()\n",
        "# attn_mask_cls = attn_mask.clone()\n",
        "\n",
        "# bs,ss,es = seq_emb.size()\n",
        "# #(s,w,d) -> (s,w,a,d/a)\n",
        "# seq_emb_t = seq_emb.view(bs,ss,attns,-1).contiguous()\n",
        "# if attns > 1:\n",
        "#   attn_mask_cls =attn_mask_cls.repeat(1,attns).view(bs,attns,-1).permute(0,-1,-2).contiguous()\n",
        "# else:\n",
        "#   attn_mask_cls = attn_mask_cls.unsqueeze(dim=-1)\n",
        "\n",
        "# # (s,w,a,d/a) -> (s,w,a,d/a,1)\n",
        "# attn_mask_cls = attn_mask_cls.unsqueeze(dim=-1)\n",
        "\n",
        "# # (s,w,a,d/a) -> (s,w,a,1)\n",
        "# alpha = attn_lyr(seq_emb_t)\n",
        "# alpha += attn_mask_cls\n",
        "# alpha = alpha.softmax(dim=-3)\n",
        "\n",
        "# print(f'[Attention-1]alpha.size()={alpha.size()}')\n",
        "# print(alpha)\n",
        "# att_s = seq_emb_t * alpha\n",
        "# print(f'att_s={att_s.size()}')\n",
        "# att_s = att_s.mean(dim=1).view(bs,-1)\n",
        "# print(f'att_s={att_s.size()}')\n",
        "# print(att_s)\n",
        "\n",
        "# #Attention - Base\n",
        "# print('Attention - Base')\n",
        "# seq_emb = i.clone()\n",
        "# attn_mask_cls = attn_mask.clone()\n",
        "# attn_mask_cls = attn_mask_cls.unsqueeze(dim=-1)\n",
        "# alpha = attn_lyr(seq_emb)\n",
        "# alpha += attn_mask_cls\n",
        "# alpha = alpha.softmax(dim=1)\n",
        "# print(alpha)\n",
        "# att_s = torch.mul(seq_emb,alpha)\n",
        "# att_s = att_s.mean(dim=1)\n",
        "# print(att_s)\n",
        "\n",
        "# Attention - 2 \n",
        "# seq_emb = i.clone()\n",
        "# attn_mask_cls = attn_mask.clone()\n",
        "\n",
        "bs,ss,es = seq_emb.size()\n",
        "#print(bs,ss,es)\n",
        "#(s,w,d) -> (s,a,w,d/a)\n",
        "seq_emb_t = seq_emb.view(bs,ss,attns,-1)\n",
        "seq_emb_t = seq_emb_t.permute(0,2,1,3)\n",
        "\n",
        "# if attns > 1:\n",
        "#   # (s,w) -> (s,a,w)\n",
        "#   attn_mask_cls = attn_mask_cls.repeat(attns,1).contiguous()\n",
        "# else:\n",
        "#   attn_mask_cls = attn_mask_cls.unsqueeze(dim=-1)\n",
        "# # (s,a,w) -> (s,a,w,1)\n",
        "# attn_mask_cls = attn_mask_cls.unsqueeze(dim=-1)\n",
        "\n",
        "# (s,a,w,d/a) -> (s,a,w,1)\n",
        "attn = attn_vec.view(attns,-1).unsqueeze(dim=-1)\n",
        "alpha = torch.matmul(seq_emb_t,attn)\n",
        "alpha /=math.sqrt(es)\n",
        "#alpha += attn_mask_cls\n",
        "alpha  = alpha + attn_mask.unsqueeze(dim=-1)\n",
        "alpha = alpha.softmax(dim=2)\n",
        "att_s = seq_emb_t*alpha\n",
        "att_s = att_s.sum(dim=-2)\n",
        "att_s = att_s.view(bs,-1).contiguous()\n",
        "print(f'[Attention-MH]alpha.size()={alpha.size()}')\n",
        "print(alpha)\n",
        "print(att_s.size())\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdbNjYOM8XEr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1 = MyModel9(attns=1)\n",
        "model1= model1.to(DEVICE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfB6nTkbGOwV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "o,a = model1.attention(seq_emb,model1.a_attn,attn_mask)\n",
        "print(attn_mask)\n",
        "print(seq_emb[0,:,:5])\n",
        "print(a)\n",
        "print(o[0,:5])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmMMnpV0Qcch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "attn_mask_cls = attn_mask.clone()\n",
        "print(attn_mask_cls.size())\n",
        "attn_mask_cls.repeat(1,attns).view(2,attns,-1).permute(0,-1,-2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DtV6cIHkIms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pytorch_transformers import BertForSequenceClassification\n",
        "############################### Model 1  ############################################\n",
        "# With different attention\n",
        "class MyModel10(nn.Module):\n",
        "  def __init__(self, freeze_bert = True, attns:int=1, attn_dropout=0.1):\n",
        "    super().__init__()\n",
        "    attns = int(attns)\n",
        "\n",
        "    self.model_version = '10'\n",
        "    \n",
        "    self.bert_lyr = BertModel.from_pretrained('bert-base-uncased',output_hidden_states=True,output_attentions=True)\n",
        "    \n",
        "    self.config = self.bert_lyr.config;\n",
        "\n",
        "    self.attns = attns\n",
        "\n",
        "    self.attn_dropout = nn.Dropout(attn_dropout)\n",
        "    \n",
        "    assert 768 % attns == 0\n",
        "    \n",
        "    att_hs = int(768 / attns)\n",
        "    \n",
        "    self.a_attn = nn.Linear(att_hs,1,bias=False)\n",
        "    \n",
        "    self.c_attn = nn.Linear(att_hs,1,bias=False)\n",
        "\n",
        "    self.a_seq_proj = nn.Linear(768,768,bias=False)\n",
        "\n",
        "    self.c_seq_proj = nn.Linear(768,768,bias=False)\n",
        "\n",
        "    self.action_cls_lyr = nn.Sequential(\n",
        "            nn.LayerNorm(768),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(768,64),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(64),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(64,4),\n",
        "    )\n",
        "    self.comp_cls_lyr = nn.Sequential(\n",
        "            nn.LayerNorm(768),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(768,64),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(64),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(64,5),\n",
        "        )    \n",
        "\n",
        "    #Freeze bert layers\n",
        "    if freeze_bert:\n",
        "        self.freeze_bert()\n",
        "    else:\n",
        "        self.freeze_bert()\n",
        "        self.unfreeze_bert()\n",
        "        \n",
        "  def freeze_bert(self):\n",
        "      for p in self.bert_lyr.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "  def unfreeze_bert(self,from_lyr=6):\n",
        "\n",
        "    for lyr in self.bert_lyr.encoder.layer[-6:]:\n",
        "      for p in lyr.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "  def attention_base(self,seq_emb,attn_lyr,attn_mask_cls):\n",
        "      attn_mask_cls = attn_mask_cls.unsqueeze(dim=-1)\n",
        "      a = attn_lyr(seq_emb)\n",
        "      a = a + attn_mask_cls\n",
        "      a = a_output = a.softmax(dim=1)\n",
        "      a = torch.mul(seq_emb,a)\n",
        "      a = a.mean(dim=1)\n",
        "      return a,a_output\n",
        "\n",
        "  def attention(self,seq_emb,attn_lyr,attn_mask_cls,seq_proj=None):\n",
        "    attns = self.attns\n",
        "    if seq_proj is not None:\n",
        "      seq_emb = seq_proj(seq_emb)\n",
        "\n",
        "    bs,ss,es = seq_emb.size()\n",
        "    #(s,w,d) -> (s,w,a,d/a)\n",
        "    seq_emb_t = seq_emb.view(bs,ss,attns,-1).contiguous()\n",
        "    if attns > 1:\n",
        "        attn_mask_cls =attn_mask_cls.repeat(1,attns).view(bs,attns,-1).permute(0,-1,-2).contiguous()\n",
        "    else:\n",
        "      attn_mask_cls = attn_mask_cls.unsqueeze(dim=-1)\n",
        "    # (s,w,a,d/a) -> (s,w,a,d/a,1)\n",
        "    attn_mask_cls = attn_mask_cls.unsqueeze(dim=-1)\n",
        "\n",
        "    # (s,w,a,d/a) -> (s,w,a,1)\n",
        "    alpha = attn_lyr(seq_emb_t)\n",
        "    alpha += attn_mask_cls\n",
        "    alpha = alpha.softmax(dim=-3)\n",
        "    alpha = self.attn_dropout(alpha)\n",
        "    #(s,w,a,d/a)\n",
        "    att_s = torch.mul(seq_emb_t, alpha)\n",
        "    att_s_ctx = att_s.sum(dim=1).view(bs,-1).contiguous()\n",
        "    alpha = alpha.squeeze(dim=-1).sum(dim=-1)\n",
        "    return att_s_ctx,alpha.squeeze(dim=-1)\n",
        "\n",
        "\n",
        "  def forward(self, seq, attn_masks,output_attn=False,output_hs=False):\n",
        "    attn_mask_cls = (1 - attn_masks)*-10000\n",
        "    #attn_mask_cls.unsqueeze_(dim=-1)\n",
        "    seq_emb,pooled,hs,attn = self.bert_lyr(seq,attention_mask =attn_masks)\n",
        "    \n",
        "    #c_seq_emb,c_pooled,c_hs,c_attn = self.comp_bert_lyr(seq,attention_mask =attn_masks)\n",
        "    a,a_output = self.attention(seq_emb,self.a_attn,attn_mask_cls,self.a_seq_proj)\n",
        "\n",
        "    c,c_output = self.attention(seq_emb,self.c_attn,attn_mask_cls,self.c_seq_proj)\n",
        "\n",
        "    a_pooled  = a\n",
        "    c_pooled  = c\n",
        "    outputs=[]\n",
        "    outputs += [\n",
        "                self.action_cls_lyr(a_pooled),\n",
        "                self.comp_cls_lyr(c_pooled),\n",
        "                ]\n",
        "    if (output_attn):\n",
        "      outputs += [a_output,c_output]\n",
        "    return outputs\n",
        "\n",
        "\n",
        "# try:\n",
        "#   del model\n",
        "#   torch.cuda.reset_max_memory_allocated()\n",
        "#   torch.cuda.reset_max_memory_cached()\n",
        "# except NameError:\n",
        "#   pass\n",
        "\n",
        "model = MyModel9(freeze_bert=False,attns=12)\n",
        "model = model.to(DEVICE)\n",
        "i = torch.rand(2,3,768,device=DEVICE)\n",
        "attn_mask = torch.zeros(2,3,device=DEVICE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s11DA4cwKnDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X, X_attn),(_,_) = next(iter(trn_dl))\n",
        "X, X_attn = X.to(DEVICE),X_attn.to(DEVICE)\n",
        "print(f'X.size()={X.size()}')\n",
        "a,c,a_a,c_a = model(X,X_attn,output_attn=True,output_hs=True)\n",
        "a_a.size()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IKDBrDGnXWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load model\n",
        "#model = MyModel5(freeze_bert=False)\n",
        "#model = MyModel4(freeze_bert=False)\n",
        "model = MyModel9(freeze_bert=False,attns=1)\n",
        "#model = MyModel10(freeze_bert=False,attns=1,attn_dropout=0.3)\n",
        "#model.load_state_dict(torch.load(f'data/model-{model.model_version}.dat'))\n",
        "model.to(DEVICE)\n",
        "\n",
        "g_wd = 3\n",
        "g_lr = 5e-4\n",
        "\n",
        "def configure_bert_optim(model:nn.Module,lr=2e-6,wd=0.1):\n",
        "      param_optimizer = list(model.named_parameters())\n",
        "      no_decay = [\"bias\", \"gamma\", \"beta\"]\n",
        "      optimizer_grouped_parameters = [\n",
        "              {\n",
        "                  \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "                  \"weight_decay_rate\": wd,\n",
        "                  \"lr\":lr,\n",
        "              },\n",
        "              {\n",
        "                  \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "                  \"weight_decay_rate\": 0.0,\n",
        "                  \"lr\":lr,\n",
        "              },\n",
        "              ]\n",
        "      return optimizer_grouped_parameters\n",
        "\n",
        "#optimizer_params = configure_bert_optim(model.bert_lyr,lr=5e-5)\n",
        "optimizer_params = [\n",
        "                                {'params':model.bert_lyr.parameters(),'lr':1e-6,'weight_decay':0.1},\n",
        "                                # {'params':model.a_attn.parameters()},\n",
        "                                # {'params':model.c_attn.parameters()},\n",
        "                                {'params':model.a_attn_vec},\n",
        "                                {'params':model.c_attn_vec},                                        \n",
        "                                {'params':model.action_cls_lyr.parameters()},\n",
        "                                {'params':model.comp_cls_lyr.parameters(),},\n",
        "                                # {'params':model.cls_lyr.parameters()},\n",
        "                     \n",
        "]\n",
        "optimizer = torch.optim.AdamW(optimizer_params,lr=g_lr,weight_decay=g_wd )\n",
        "scheduler = CosineAnnealingWarmRestarts(optimizer,T_0=20,T_mult=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_XdOgy_DHCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train model\n",
        "# torch.cuda.reset_max_memory_allocated()\n",
        "# torch.cuda.reset_max_memory_cached()\n",
        "SAVE_TO_FILE=True\n",
        "print(f'Device type is {DEVICE.type}')\n",
        "print(f'Runnig model version {model.model_version}')\n",
        "model.to(DEVICE)\n",
        "model.train()\n",
        "le_trnf = action_le.transform(trn_df.action)\n",
        "u,c = np.unique(le_trnf,return_counts=True)\n",
        "ac_class_weight=compute_class_weight('balanced',classes=u,y=le_trnf)\n",
        "ac_class_weight = torch.tensor(ac_class_weight,dtype=torch.float,device=DEVICE)\n",
        "le_trnf = component_le.transform(trn_df.component)\n",
        "u,c = np.unique(le_trnf,return_counts=True)\n",
        "com_class_weight=compute_class_weight('balanced',classes=u,y=le_trnf)\n",
        "com_class_weight = torch.tensor(com_class_weight,dtype=torch.float,device=DEVICE)\n",
        "\n",
        "print(f'ac_class_weight={ac_class_weight}')\n",
        "print(f'com_class_weight={com_class_weight}')\n",
        "\n",
        "\n",
        "action_criterion = nn.modules.loss.CrossEntropyLoss(weight=ac_class_weight,reduction='mean')\n",
        "component_criterion = nn.modules.loss.CrossEntropyLoss(weight=com_class_weight,reduction='mean')\n",
        "\n",
        "n_epochs = 1000\n",
        "\n",
        "def evaluate_model(epoch:int,model:nn.Module,dl:DataLoader,optimizer):\n",
        "  t_loss=0.0\n",
        "  t_a_loss=0.0\n",
        "  t_c_loss=0.0\n",
        "  iters = len(dl)\n",
        "  for i,((X, attn_mask),(Y1,Y2)) in enumerate(dl):\n",
        "    X , attn_mask,Y1,Y2 = X.to(DEVICE),attn_mask.to(DEVICE),Y1.to(DEVICE),Y2.to(DEVICE)\n",
        "    p_a,p_c = model(X,attn_mask)\n",
        "    #print(f'p_a size={p_a.size()}')\n",
        "    action_loss = action_criterion(p_a.view(-1,4),Y1)\n",
        "    component_loss = component_criterion(p_c.view(-1,5),Y2)\n",
        "    loss =   action_loss + component_loss\n",
        "    if optimizer is not None:\n",
        "      scheduler.step(epoch+i/iters)\n",
        "      optimizer.zero_grad()\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(),5)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    t_loss += loss.item()\n",
        "    t_a_loss += action_loss.item()\n",
        "    t_c_loss += component_loss.item()\n",
        "    \n",
        "  return t_loss,t_a_loss,t_c_loss\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  t_loss=0\n",
        "  a_loss=0\n",
        "  c_loss=0\n",
        "  t_loss,a_loss,c_loss = evaluate_model(epoch,model,trn_dl,optimizer)\n",
        "  if SAVE_TO_FILE and epoch%10 == 0:\n",
        "    torch.save(model.state_dict(),f'data/model-{model.model_version}.dat')\n",
        "  v_loss = 0\n",
        "  with torch.no_grad():\n",
        "    v_loss = evaluate_model(epoch,model,val_dl,None)\n",
        "    v_loss = [f'{v:0.4}' for v in v_loss]\n",
        "  print(f'Epoch:{epoch} Trn loss={t_loss:0.4}, Actn loss:{a_loss:0.4} ,Comp loss:{c_loss:0.4},  Validation loss:{v_loss}')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4TEkKZhNWSl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_tensor(t:torch.tensor):\n",
        "  t = t.squeeze().detach().cpu().numpy()\n",
        "  t = [f'{item:0.3}' for item in t]\n",
        "  print(t)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  X,X_attn_mask = encode_X('',max_len=20)\n",
        "  \n",
        "  X_tokenized = X\n",
        "  X,X_attn_mask = X.to(DEVICE), X_attn_mask.to(DEVICE)\n",
        "  X.unsqueeze_(0)\n",
        "  X_attn_mask.unsqueeze_(0)\n",
        "  action,component,a_attn,c_attn = model(X,X_attn_mask,output_attn=True)\n",
        "  #action,component = model(X,X_attn_mask,output_attn=False)\n",
        "  action = action.softmax(dim=1)\n",
        "  component = component.softmax(dim=1)\n",
        "  action,component = [a.squeeze().detach().cpu().numpy() for a in [action,component]]\n",
        "\n",
        "  print(action_le.classes_)  \n",
        "  print(action)\n",
        "  print(a_attn.size())\n",
        "  action = np.argmax(action)\n",
        "  print(f'Selected Action: {action_le.inverse_transform([action])}')\n",
        "\n",
        "  print(component_le.classes_)\n",
        "  print(component)\n",
        "  component = np.argmax(component)\n",
        "  print(component)\n",
        "  print(f'Selected component: {component_le.inverse_transform([component])}')\n",
        "\n",
        "  print(tokenizer.convert_ids_to_tokens(X_tokenized.numpy().squeeze()))\n",
        "  print_tensor(a_attn)\n",
        "  print_tensor(c_attn)\n",
        "  # print(a_attn.squeeze().detach().cpu().numpy())\n",
        "  # print(c_attn.squeeze().detach().cpu().numpy())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8JOQMxA5AqW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.a_attn_vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UGbEM_6fjB-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.a_attn_vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4mB7LOFJmj7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}